# -*- coding: utf-8 -*-
"""Assignment_5_student.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aZph-yuSmdr9KkAKfo7Q18dSEN2KBLvn
"""

"""CIFAR 10 Part"""


from __future__ import absolute_import
import numpy as np
import os
import sys
import six
from six.moves import cPickle
from six.moves.urllib.error import HTTPError
from six.moves.urllib.error import URLError
from six.moves.urllib.request import urlopen


def get_file(fname,
             origin,
             untar=False,
             md5_hash=None,
             file_hash=None,
             cache_subdir='datasets',
             hash_algorithm='auto',
             extract=False,
             archive_format='auto',
             cache_dir=None):
    
    if cache_dir is None:
        cache_dir = os.path.expanduser(os.path.join('~', '.keras'))
    if md5_hash is not None and file_hash is None:
        file_hash = md5_hash
        hash_algorithm = 'md5'
    datadir_base = os.path.expanduser(cache_dir)
    if not os.access(datadir_base, os.W_OK):
        datadir_base = os.path.join('/tmp', '.keras')
    datadir = os.path.join(datadir_base, cache_subdir)
    if not os.path.exists(datadir):
        os.makedirs(datadir)

    if untar:
        untar_fpath = os.path.join(datadir, fname)
        fpath = untar_fpath + '.tar.gz'
    else:
        fpath = os.path.join(datadir, fname)

    download = False
    if os.path.exists(fpath):
        # File found; verify integrity if a hash was provided.
        if file_hash is not None:
            if not validate_file(fpath, file_hash, algorithm=hash_algorithm):
                print('A local file was found, but it seems to be '
                      'incomplete or outdated because the ' + hash_algorithm +
                      ' file hash does not match the original value of ' +
                      file_hash + ' so we will re-download the data.')
                download = True
    else:
        download = True

    if download:
        print('Downloading data from', origin)

        class ProgressTracker(object):
            # Maintain progbar for the lifetime of download.
            # This design was chosen for Python 2.7 compatibility.
            progbar = None

        def dl_progress(count, block_size, total_size):
            if ProgressTracker.progbar is None:
                if total_size is -1:
                    total_size = None
                ProgressTracker.progbar = Progbar(total_size)
            else:
                ProgressTracker.progbar.update(count * block_size)

        error_msg = 'URL fetch failure on {}: {} -- {}'
        try:
            try:
                urlretrieve(origin, fpath, dl_progress)
            except URLError as e:
                raise Exception(error_msg.format(origin, e.errno, e.reason))
            except HTTPError as e:
                raise Exception(error_msg.format(origin, e.code, e.msg))
        except (Exception, KeyboardInterrupt) as e:
            if os.path.exists(fpath):
                os.remove(fpath)
            raise
        ProgressTracker.progbar = None

    if untar:
        if not os.path.exists(untar_fpath):
            _extract_archive(fpath, datadir, archive_format='tar')
        return untar_fpath

    if extract:
        _extract_archive(fpath, datadir, archive_format)

    return fpath


def load_batch(fpath, label_key='labels'):
    """Internal utility for parsing CIFAR data.
    # Arguments
        fpath: path the file to parse.
        label_key: key for label data in the retrieve
            dictionary.
    # Returns
        A tuple `(data, labels)`.
    """
    f = open(fpath, 'rb')
    if sys.version_info < (3,):
        d = cPickle.load(f)
    else:
        d = cPickle.load(f, encoding='bytes')
        # decode utf8
        d_decoded = {}
        for k, v in d.items():
            d_decoded[k.decode('utf8')] = v
        d = d_decoded
    f.close()
    data = d['data']
    labels = d[label_key]

    data = data.reshape(data.shape[0], 3, 32, 32)
    return data, labels


def load_data():
    """Loads CIFAR10 dataset.
    # Returns
        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.
    """
    dirname = 'cifar-10-batches-py'
    origin = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'
   # path = get_file(dirname, origin=origin, untar=True)

    num_train_samples = 50000

    cifar10 = tf.keras.datasets.cifar10
    (x_train, y_train), (x_test, y_test) = cifar10.load_data()
    print(x_train.shape, x_test.shape)
    #print(x_train.shape)
    y_train = np.reshape(y_train, (len(y_train), 1))
    y_test = np.reshape(y_test, (len(y_test), 1))

    return (x_train, y_train), (x_test, y_test)


class Cifar10(object):
  def __init__(self, batch_size=64, one_hot=False, test=False, shuffle=True):
    (x_train, y_train), (x_test, y_test) = load_data()
    if test:
      images = x_test
      labels = y_test
    else:
      images = x_train
      labels = y_train
    if one_hot:
      one_hot_labels = np.zeros((len(labels), 10))
      one_hot_labels[np.arange(len(labels)), labels.flatten()] = 1
      labels = one_hot_labels
    self.shuffle = shuffle
    self._images = images
    self.images = self._images
    self._labels = labels
    self.labels = self._labels
    self.batch_size = batch_size
    self.num_samples = len(self.images)
    if self.shuffle:
      self.shuffle_samples()
    self.next_batch_pointer = 0

  def shuffle_samples(self):
    image_indices = np.random.permutation(np.arange(self.num_samples))
    self.images = self._images[image_indices]
    self.labels = self._labels[image_indices]

  def get_next_batch(self):
    num_samples_left = self.num_samples - self.next_batch_pointer
    if num_samples_left >= self.batch_size:
      x_batch = self.images[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]
      y_batch = self.labels[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]
      self.next_batch_pointer += self.batch_size
    else:
      x_partial_batch_1 = self.images[self.next_batch_pointer:self.num_samples]
      y_partial_batch_1 = self.labels[self.next_batch_pointer:self.num_samples]
      if self.shuffle:
        self.shuffle_samples()
      x_partial_batch_2 = self.images[0:self.batch_size - num_samples_left]
      y_partial_batch_2 = self.labels[0:self.batch_size - num_samples_left]
      x_batch = np.vstack((x_partial_batch_1, x_partial_batch_2))
      y_batch = np.vstack((y_partial_batch_1, y_partial_batch_2))
      self.next_batch_pointer = self.batch_size - num_samples_left
    return x_batch, y_batch

"""Ops part"""


# Reference: https://github.com/openai/improved-gan/blob/master/imagenet/ops.py
import tensorflow as tf
import numpy as np


# TODO: Optionally create utility functions for convolution, fully connected layers here

"""net part"""

from tensorflow.contrib.layers import flatten
from sklearn.utils import shuffle

def net(x, isTrain, dropout_kept_prob):
  # 0.03 sigma and 0.97 decay rate 
  # Hyperparameters
  mu = 0
  sigma = 0.03
  
  # output size = (N - F) / stride + 1
    
  # dropout for regularization, reduces overfitting, randomly selected neurons are ignored, 
  # meaning that their contribution to the activation of downstream neurons 
  # is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.
  # layers help recognize visual patterns from images
  
  # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.
  conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma), name='conv1_W')
  conv1_b = tf.Variable(tf.zeros(6), name='conv1_b')
  # apply filter to each window of each image 
  # valid padding does not do 0 pixel padding
  # same padding adds 0 padding so that output feature map size is same as input feature map size
  conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='SAME') + conv1_b # notice the padding option "SAME"

  # Batch normalization
  # helps the gradient by trying to change the mean to 0 
  conv1 = tf.layers.batch_normalization(conv1,training=isTrain)

  # Relu Activation.
  conv1 = tf.nn.relu(conv1)


  # Densenet-like connection: Convolutional. Output = 32x32x16.
  conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 9, 16), mean = mu, stddev = sigma), name='conv2_W')
  conv2_b = tf.Variable(tf.zeros(16), name='conv2_b')
  # skip connection
  conv2   = tf.nn.conv2d(tf.concat([x,conv1],axis=3), conv2_W, strides=[1, 1, 1, 1], padding='SAME') + conv2_b

  # Relu Activation.
  conv2 = tf.layers.batch_normalization(conv2,training=isTrain)
  conv2 = tf.nn.relu(conv2)
  
  

  # Pooling to reduce dimension. Input = 32x32x16. Output = 8x8x16.
  # ksize is the size of the window for each dimention of input tensor
  # progressively reduce the spatial size of the representation--downsampling--
  # to reduce the amount of parameters and computation in the network, and hence to also control overfitting  
  conv2 = tf.nn.max_pool(conv2, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='VALID')

  # Flatten before full connection. Input = 8x8x16. Output = 1024.
  fc0   = flatten(conv2)

  # Dropout 
  DropMask = (tf.to_float(tf.random_uniform((1,1024))<dropout_kept_prob))/dropout_kept_prob
  fc0 = tf.cond(isTrain, lambda: fc0*DropMask, lambda: fc0)


  # SOLUTION: Layer 3: Fully Connected. Input = 1024. Output = 120.
  fc1_W = tf.Variable(tf.truncated_normal(shape=(1024, 120), mean = mu, stddev = sigma), name='fc1_W')
  fc1_b = tf.Variable(tf.zeros(120), name='fc1_b')
  fc1   = tf.matmul(fc0, fc1_W) + fc1_b

  # SOLUTION: Activation.
  fc1 = tf.layers.batch_normalization(fc1,training=isTrain)
  fc1    = tf.nn.relu(fc1)
  
  # dropout for layer 3 
#   DropMask = (tf.to_float(tf.random_uniform((1,120))<dropout_kept_prob))/dropout_kept_prob
#   fc1 = tf.cond(isTrain, lambda: fc1*DropMask, lambda: fc1)
  
  
  # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.
  fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma), name='fc2_W')
  fc2_b  = tf.Variable(tf.zeros(84), name='fc2_b')
  fc2    = tf.matmul(fc1, fc2_W) + fc2_b

  # SOLUTION: Activation.
  fc2 = tf.layers.batch_normalization(fc2,training=isTrain)
  fc2    = tf.nn.relu(fc2)
  
  
  DropMask = (tf.to_float(tf.random_uniform((1,84))<dropout_kept_prob))/dropout_kept_prob
  fc2 = tf.cond(isTrain, lambda: fc2*DropMask, lambda: fc2)

  # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.
  fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma), name='fc3_W')
  fc3_b  = tf.Variable(tf.zeros(10), name='fc3_b')
  logits = tf.matmul(fc2, fc3_W) + fc3_b

  return logits

def train():
  # Always use tf.reset_default_graph() to avoid error
  tf.reset_default_graph()
  
  EPOCHS = 100
  batches = 110
  lr = 0.007
  
  x = tf.placeholder(tf.float32, (None, 32, 32, 3))
  y = tf.placeholder(tf.int32, (None))
  # arrays of 0's with a 1 indexing the label 
  one_hot_y = tf.one_hot(y, 10)
  train = tf.placeholder(tf.bool)
  logits = net(x, train, 0.8)
  
  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)
  loss_operation = tf.reduce_mean(cross_entropy)
  
  global_step = tf.Variable(0, trainable=True)
  step = tf.assign(global_step, global_step + 1)
  lr = tf.train.exponential_decay(lr, global_step, 100, 0.97, staircase=True)
  optimizer = tf.train.AdamOptimizer(learning_rate = lr)
  
  grads_and_vars = optimizer.compute_gradients(loss_operation, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))


  
  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # to take care of Dropout layer
  with tf.control_dependencies(update_ops):
    training_operation = optimizer.apply_gradients(grads_and_vars)
  
  cifar10_images = Cifar10()
  # only keep one checkpoint file
  saver = tf.train.Saver(max_to_keep=0)

  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    tf.contrib.layers.xavier_initializer()
    for i in range(EPOCHS):
      for j in range(batches):
        x_batch, y_batch = cifar10_images.get_next_batch()  
#         learn = sess.run(lr, feed_dict={x: x_batch, y: y_batch, train: True})
        sess.run(training_operation, feed_dict={x: x_batch, y: y_batch, train: True})  
        sess.run(step)
#         loss = sess.run(loss_operation, feed_dict={x: x_batch, y: y_batch, train: True})
#         print(i, loss)
    # save trained model 
    saver.save(sess, './model')  


def test(cifar10_test_images):
  # Always use tf.reset_default_graph() to avoid error
  tf.reset_default_graph()
  
  x = tf.placeholder(tf.float32, (None, 32, 32, 3))
  y = tf.placeholder(tf.int32, (None))
  one_hot_y = tf.one_hot(y, 10)
  train = tf.placeholder(tf.bool)
  logits = net(x, train, 0.8)
  saver = tf.train.Saver(max_to_keep=0)

  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    tf.contrib.layers.xavier_initializer()
    saver.restore(sess, './model')
    test = sess.run(logits, feed_dict={x: cifar10_test_images, train: False})
    # find best labels 
    y_test = np.argmax(test, axis=1)
    
    return y_test

import numpy as np
import timeit
from collections import OrderedDict
from pprint import pformat

min_thres=0.60
max_thres=0.80


def compute_score(acc, min_thres, max_thres):
    if acc <= min_thres:
        base_score = 0.0
    elif acc >= max_thres:
        base_score = 100.0
    else:
        base_score = float(acc - min_thres) / (max_thres - min_thres) \
            * 100
    return base_score


if __name__ == '__main__':
    TRAIN = True
    if TRAIN:
        train()
    cifar10_test = Cifar10(test=True, shuffle=False, one_hot=False)
    cifar10_test_images, cifar10_test_labels = cifar10_test._images, cifar10_test._labels

    start = timeit.default_timer()
    np.random.seed(0)
    predicted_cifar10_test_labels = test(cifar10_test_images)
    np.random.seed()
    stop = timeit.default_timer()
    run_time = stop - start
    correct_predict = (cifar10_test_labels.flatten() == predicted_cifar10_test_labels.flatten()).astype(np.int32).sum()
    incorrect_predict = len(cifar10_test_labels) - correct_predict
    accuracy = float(correct_predict) / len(cifar10_test_labels)
    print('Acc: {}. Testing took {}s.'.format(accuracy, stop - start))
    
    score = compute_score(accuracy, min_thres, max_thres)
    result = OrderedDict(correct_predict=correct_predict,
                     accuracy=accuracy, score=score,
                     run_time=run_time)

    with open('result.txt', 'w') as f:
        f.writelines(pformat(result, indent=4))
    print(pformat(result, indent=4))

